{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f546f2fa-7677-4ef7-8c0f-8af1620eeeb4",
   "metadata": {},
   "source": [
    "# Training RGCN Recommender on INSPIRED Dataset\n",
    "\n",
    "This notebook trains the RGCN-based movie recommender on the INSPIRED dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113361b-8f64-4108-ae20-cb35bb2abb7d",
   "metadata": {},
   "source": [
    "## Environmental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1290935e-6700-47fa-b8da-8786eb5a4065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\91953\\Documents\\GitHub\\RAG-Movie-CRS\n",
      "Current Working Directory: C:\\Users\\91953\\Documents\\GitHub\\RAG-Movie-CRS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Check current directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Project Root:\", project_root)\n",
    "print(\"Current Working Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36d4193-8b3c-4c85-9661-ee7d4296ab85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: ollama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: llama-index-core in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.14.8)\n",
      "Requirement already satisfied: llama-index-embeddings-ollama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (0.8.4)\n",
      "Requirement already satisfied: llama-index-llms-ollama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: chromadb in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: pypdf in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (6.4.0)\n",
      "Requirement already satisfied: nbimport in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (0.0.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (4.57.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (1.7.2)\n",
      "Requirement already satisfied: torch in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (0.24.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from ollama->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from ollama->-r requirements.txt (line 2)) (2.12.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from tqdm->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (3.13.2)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2025.10.0)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2.11.5)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (3.9.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (10.3.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 4)) (2.0.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (1.22.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\91953\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core->-r requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (3.11)\n",
      "Requirement already satisfied: pytest-asyncio>=0.23.8 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-embeddings-ollama->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (0.38.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (3.11.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 8)) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 4)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 4)) (2025.11.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 13)) (1.14.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\91953\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\91953\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91953\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 8)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 8)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 8)) (0.28.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (0.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\91953\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core->-r requirements.txt (line 4)) (8.3.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\91953\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\91953\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (25.9.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\91953\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (6.33.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 8)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 8)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 8)) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 8)) (0.59b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 2)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: pytest<10,>=8.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (9.0.0)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 4)) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 8)) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (1.2.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27->ollama->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (3.5.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core->-r requirements.txt (line 4)) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (3.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48e6bb7-9f2a-4bb7-87d7-7f8441dcfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import NCF model and evaluator\n",
    "from scripts.ncf_recommender import NCFModel, INSPIREDDataProcessor, train_ncf\n",
    "from scripts.evaluator import RecommenderEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf82d16-0ac2-4bf2-9b18-8952b1487052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04be16-4e70-465d-8b6b-b1f7f2a4c577",
   "metadata": {},
   "source": [
    "## DATA PREPARATION FOR TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f183a4e-3e16-4111-b152-decc1f6e62d3",
   "metadata": {},
   "source": [
    "### Load Dataset And Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7760b026-9b0a-43e4-9b8c-5e5d1621d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: LOAD AND PROCESS DATA\n",
      "============================================================\n",
      "\n",
      "Loading movie database...\n",
      "Loading 17869 movies from database...\n",
      "Loaded 16764 movies\n",
      "\n",
      "Loading training dialogs...\n",
      "\n",
      "============================================================\n",
      "Loading interactions from train.tsv\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 801/801 [00:52<00:00, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interaction Summary:\n",
      "  Matched: 3061\n",
      "  Unmatched: 1252\n",
      "  Match rate: 71.0%\n",
      "============================================================\n",
      "\n",
      "\n",
      "Data Statistics:\n",
      "  Users: 801\n",
      "  Movies: 16764\n",
      "  Interactions: 3061\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 1: LOAD AND PROCESS DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize data processor\n",
    "dataset_dir = \"data\"\n",
    "data_processor = INSPIREDDataProcessor(dataset_dir=dataset_dir)\n",
    "\n",
    "# Load movie database\n",
    "print(\"\\nLoading movie database...\")\n",
    "movie_df = data_processor.load_movie_database()\n",
    "\n",
    "# Load training dialogs\n",
    "print(\"\\nLoading training dialogs...\")\n",
    "data_processor.load_dialogs(split=\"train\", max_dialogs=None)\n",
    "\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"  Users: {data_processor.get_num_users()}\")\n",
    "print(f\"  Movies: {data_processor.get_num_items()}\")\n",
    "print(f\"  Interactions: {len(data_processor.interactions)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16628e-2620-4e92-bd78-318e8baa0751",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6382bd0-ec40-4a41-ba1c-ec31a1f3027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: INITIALIZE NCF MODEL\n",
      "============================================================\n",
      "Using device: cpu\n",
      "\n",
      "\n",
      "Data Analysis:\n",
      "  Total movies in database: 16764\n",
      "  Unique items in training: 1037\n",
      "  Max item index: 17816\n",
      "  Using max_item_idx + 1 = 17817 for embeddings\n",
      "\n",
      "Model Configuration:\n",
      "  Users: 801\n",
      "  Items: 16764\n",
      "  Embedding dim: 64\n",
      "  MLP layers: [128, 64, 32]\n",
      "  Total parameters: 2,410,049\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: INITIALIZE NCF MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_DIM = 64\n",
    "MLP_LAYERS = [128, 64, 32]\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Get actual unique items in training data\n",
    "users, items, ratings = data_processor.get_train_data()\n",
    "unique_items = set(items)\n",
    "num_unique_items = len(unique_items)\n",
    "max_item_idx = max(items)\n",
    "\n",
    "print(f\"\\nData Analysis:\")\n",
    "print(f\"  Total movies in database: {data_processor.get_num_items()}\")\n",
    "print(f\"  Unique items in training: {num_unique_items}\")\n",
    "print(f\"  Max item index: {max_item_idx}\")\n",
    "print(f\"  Using max_item_idx + 1 = {max_item_idx + 1} for embeddings\\n\")\n",
    "\n",
    "# Use max index + 1 for embedding size\n",
    "model = NCFModel(\n",
    "    num_users=data_processor.get_num_users(),\n",
    "    num_items=max_item_idx + 1,  # ← FIXED\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    mlp_layers=MLP_LAYERS\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  Users: {data_processor.get_num_users()}\")\n",
    "print(f\"  Items: {data_processor.get_num_items()}\")\n",
    "print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
    "print(f\"  MLP layers: {MLP_LAYERS}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b3e94-11d7-41b4-b220-9e0e2d884bd4",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dce1c1-2844-4e3d-a16e-7df9a9391f27",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d55866-b8fa-41bd-9931-458296ca583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: TRAINING NCF MODEL\n",
      "============================================================\n",
      "\n",
      "Training Configuration:\n",
      "  Epochs: 10\n",
      "  Batch size: 256\n",
      "  Learning rate: 0.001\n",
      "  Negative samples: 4\n",
      "  Checkpoint dir: C:\\Users\\91953\\Documents\\GitHub\\RAG-Movie-CRS\\models\\ncf_checkpoints\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "TRAINING NCF MODEL\n",
      "============================================================\n",
      "Total interactions: 3061\n",
      "Users: 801\n",
      "Items: 16764\n",
      "Unique items in training: 1037\n",
      "Negative samples per positive: 4\n",
      "============================================================\n",
      "Epoch 1/10 - Loss: 0.6722\n",
      "Epoch 2/10 - Loss: 0.5800\n",
      "Epoch 3/10 - Loss: 0.4739\n",
      "Epoch 4/10 - Loss: 0.4430\n",
      "Epoch 5/10 - Loss: 0.4309\n",
      "Epoch 6/10 - Loss: 0.4229\n",
      "Epoch 7/10 - Loss: 0.4134\n",
      "Epoch 8/10 - Loss: 0.4088\n",
      "Epoch 9/10 - Loss: 0.4012\n",
      "Epoch 10/10 - Loss: 0.3932\n",
      "============================================================\n",
      "Training complete!\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get project root\n",
    "project_root = Path(os.getcwd())\n",
    "if project_root.name == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: TRAINING NCF MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NEGATIVE_SAMPLES = 4\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = project_root / \"models\" / \"ncf_checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Negative samples: {NEGATIVE_SAMPLES}\")\n",
    "print(f\"  Checkpoint dir: {checkpoint_dir}\")\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = train_ncf(\n",
    "    model=model,\n",
    "    data_processor=data_processor,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    negative_samples=NEGATIVE_SAMPLES,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c861c41-3451-4550-95f0-9dc1830f107d",
   "metadata": {},
   "source": [
    "NCF limitation: Cannot evaluate on completely new users (cold-start problem)\n",
    "<br> Solution: Split each user's interactions into train/test, not users\n",
    "This is why Transformer + RAG is better for conversational systems: it handles new users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3923b432-4645-4ee7-9fc0-29fefe7fb5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: RUN EVALUATION WITH PROPER SPLIT\n",
      "Using 1037 items from training\n",
      "\n",
      "Evaluation setup:\n",
      "  Users in evaluation: 654\n",
      "  Train interactions per user: 2.6 avg\n",
      "  Test interactions per user: 1.3 avg\n",
      "\n",
      "Evaluating on 100 users...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 100/100 [00:00<00:00, 364.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Dataset: NCF Test Set (Same Users, Held-out Items)\n",
      "Number of samples: 100\n",
      "\n",
      "HIT:\n",
      "  @ 1: 0.0600\n",
      "  @ 3: 0.1600\n",
      "  @ 5: 0.1600\n",
      "  @10: 0.2600\n",
      "\n",
      "MRR:\n",
      "  @ 1: 0.0600\n",
      "  @ 3: 0.1067\n",
      "  @ 5: 0.1067\n",
      "  @10: 0.1188\n",
      "\n",
      "NDCG:\n",
      "  @ 1: 0.0600\n",
      "  @ 3: 0.1001\n",
      "  @ 5: 0.1001\n",
      "  @10: 0.1298\n",
      "\n",
      "Recall:\n",
      "  @ 1: 0.0483\n",
      "  @ 3: 0.1233\n",
      "  @ 5: 0.1233\n",
      "  @10: 0.2133\n",
      "\n",
      "Results saved to: C:\\Users\\91953\\Documents\\GitHub\\RAG-Movie-CRS\\models\\ncf_checkpoints\\evaluation_results.json\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 4: RUN EVALUATION WITH PROPER SPLIT\")\n",
    "\n",
    "# Get ALL training interactions\n",
    "users, items, ratings = data_processor.get_train_data()\n",
    "unique_items_in_training = list(set(items))\n",
    "\n",
    "all_movie_indices = torch.tensor(\n",
    "    unique_items_in_training,\n",
    "    dtype=torch.long\n",
    ").to(device)\n",
    "\n",
    "print(f\"Using {len(all_movie_indices)} items from training\")\n",
    "\n",
    "# FIXED: Create train/test split on SAME users\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Group interactions by user\n",
    "user_interactions = defaultdict(list)\n",
    "for u, i, r in zip(users, items, ratings):\n",
    "    user_interactions[u].append((i, r))\n",
    "\n",
    "# Split each user's interactions 80/20\n",
    "train_ground_truth = {}\n",
    "test_ground_truth = {}\n",
    "\n",
    "for user_idx, interactions in user_interactions.items():\n",
    "    if len(interactions) < 2:  # Skip users with only 1 interaction\n",
    "        continue\n",
    "    \n",
    "    # Shuffle interactions\n",
    "    interactions = list(interactions)\n",
    "    random.shuffle(interactions)\n",
    "    \n",
    "    # 80/20 split\n",
    "    split_point = int(0.8 * len(interactions))\n",
    "    train_items = [item for item, rating in interactions[:split_point]]\n",
    "    test_items = [item for item, rating in interactions[split_point:]]\n",
    "    \n",
    "    if test_items:  # Only include users with test items\n",
    "        train_ground_truth[user_idx] = set(train_items)\n",
    "        test_ground_truth[user_idx] = set(test_items)\n",
    "\n",
    "print(f\"\\nEvaluation setup:\")\n",
    "print(f\"  Users in evaluation: {len(test_ground_truth)}\")\n",
    "print(f\"  Train interactions per user: {sum(len(v) for v in train_ground_truth.values()) / len(train_ground_truth):.1f} avg\")\n",
    "print(f\"  Test interactions per user: {sum(len(v) for v in test_ground_truth.values()) / len(test_ground_truth):.1f} avg\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RecommenderEvaluator(k_values=[1, 3, 5, 10])\n",
    "\n",
    "# Evaluate\n",
    "test_users = list(test_ground_truth.keys())[:100]\n",
    "print(f\"\\nEvaluating on {len(test_users)} users...\\n\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for user_idx in tqdm(test_users, desc=\"Generating predictions\"):\n",
    "    # Get recommendations\n",
    "    recommendations = trained_model.predict_top_k(\n",
    "        user_idx=user_idx,\n",
    "        candidate_items=all_movie_indices,\n",
    "        k=10\n",
    "    )\n",
    "    \n",
    "    recommended_ids = [rec['item_idx'] for rec in recommendations]\n",
    "    \n",
    "    predictions.append({\n",
    "        'recommended': recommended_ids,\n",
    "        'ground_truth': test_ground_truth[user_idx]\n",
    "    })\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nComputing metrics...\")\n",
    "results = evaluator.evaluate_batch(predictions)\n",
    "\n",
    "# Print results\n",
    "evaluator.print_results(\n",
    "    dataset_name=\"NCF Test Set (Same Users, Held-out Items)\",\n",
    "    num_samples=len(predictions)\n",
    ")\n",
    "\n",
    "# Save results\n",
    "evaluator.save_results(\n",
    "    output_path=str(checkpoint_dir / \"evaluation_results.json\"),\n",
    "    model_name=\"NCF\",\n",
    "    metadata={\n",
    "        'num_users': len(test_ground_truth),\n",
    "        'num_items': len(unique_items_in_training),\n",
    "        'test_samples': len(predictions),\n",
    "        'evaluation_type': 'same_users_holdout_items'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692216f-6559-4fcc-b069-050b696d7ba7",
   "metadata": {},
   "source": [
    "### Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5df800-343d-4268-8c77-a04429d89e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = checkpoint_dir / \"ncf_model.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'num_users': data_processor.get_num_users(),\n",
    "    'num_items': data_processor.get_num_items(),\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'mlp_layers': MLP_LAYERS,\n",
    "    'user_to_idx': data_processor.user_to_idx,\n",
    "    'idx_to_user': data_processor.idx_to_user,\n",
    "    'movie_to_idx': data_processor.movie_to_idx,\n",
    "    'idx_to_movie': data_processor.idx_to_movie,\n",
    "    'training_config': {\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'negative_samples': NEGATIVE_SAMPLES\n",
    "    }\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"✓ Model saved to: {checkpoint_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = checkpoint_dir / \"training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"✓ Training history saved to: {history_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed7834-a26e-4795-899b-508b01865382",
   "metadata": {},
   "source": [
    "### VIZ TRAINING HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3824e0b-a0ae-4c1b-9a34-0c33e2fe0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: VISUALIZE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot training loss\n",
    "epochs = history['epoch']\n",
    "losses = history['train_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, losses, marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('NCF Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plot_path = checkpoint_dir / \"training_loss.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Training plot saved to: {plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c95948-2ee4-4a1e-8774-b84947669d64",
   "metadata": {},
   "source": [
    "## PLOT TRAINING HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2c854-3479-4b87-83d4-499c136dba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Install plotly if needed\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "except ImportError:\n",
    "    print(\"Installing plotly...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'plotly'])\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "# Load the saved history\n",
    "history_path = checkpoint_dir / \"training_history.json\"\n",
    "\n",
    "if not history_path.exists():\n",
    "    print(f\"File not found: {history_path}\")\n",
    "else:\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch_idx = history['train_loss'].index(min(history['train_loss']))\n",
    "    best_epoch = history['epoch'][best_epoch_idx]\n",
    "    best_loss = history['train_loss'][best_epoch_idx]\n",
    "    \n",
    "    # Create interactive plot with Plotly\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add training loss line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=history['epoch'],\n",
    "        y=history['train_loss'],\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss',\n",
    "        line=dict(color='blue', width=2),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "    \n",
    "    # Add best epoch marker\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[best_epoch],\n",
    "        y=[best_loss],\n",
    "        mode='markers',\n",
    "        name=f'Best: Epoch {best_epoch}',\n",
    "        marker=dict(size=15, color='red', symbol='star')\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='RGCN Training Loss Over Epochs',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title='Loss',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',\n",
    "        width=1000,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Save as interactive HTML\n",
    "    html_path = Path(\"models/rgcn_checkpoints/training_plot.html\")\n",
    "    fig.write_html(str(html_path))\n",
    "    print(f\"Interactive plot saved to: {html_path}\")\n",
    "    print(f\"  Open this file in your web browser to view the plot\")\n",
    "    \n",
    "    # Also save as static image (PNG)\n",
    "    try:\n",
    "        png_path = Path(\"models/rgcn_checkpoints/training_plot.png\")\n",
    "        fig.write_image(str(png_path))\n",
    "        print(f\"Static plot saved to: {png_path}\")\n",
    "    except:\n",
    "        print(\"  (PNG export requires kaleido: pip install kaleido)\")\n",
    "    \n",
    "    # Display in notebook (should be stable)\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nBest epoch: {best_epoch} with loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b747866-2a90-4572-b3a0-f976ed409c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: TEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test on sample user\n",
    "sample_users = list(data_processor.user_to_idx.keys())[:3]\n",
    "\n",
    "for user_id in sample_users:\n",
    "    user_idx = data_processor.user_to_idx[user_id]\n",
    "    \n",
    "    print(f\"\\nUser: {user_id}\")\n",
    "    print(f\"Top-5 Recommendations:\")\n",
    "    \n",
    "    recommendations = trained_model.predict_top_k(\n",
    "        user_idx=user_idx,\n",
    "        candidate_items=all_movie_indices,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        movie_name = data_processor.idx_to_movie[rec['item_idx']]\n",
    "        print(f\"  {i}. {movie_name} (score: {rec['score']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete! Run NCF.ipynb for evaluation.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6b26a-e346-4186-9580-791bb34c14f5",
   "metadata": {},
   "source": [
    "## Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7cbc6-0f87-4710-8e7a-1120e13416c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 6: TESTING TRAINED MODEL\")\n",
    "\n",
    "# Get sample users for testing\n",
    "sample_user_ids = list(graph_builder.user_to_idx.keys())\n",
    "\n",
    "if len(sample_user_ids) > 0:\n",
    "    # Test on first 3 users\n",
    "    num_test_users = min(3, len(sample_user_ids))\n",
    "    \n",
    "    print(f\"\\nTesting recommendations for {num_test_users} users...\\n\")\n",
    "    \n",
    "    # Get all movie indices as candidates\n",
    "    all_movie_indices = torch.tensor(\n",
    "        list(graph_builder.movie_to_idx.values()),\n",
    "        dtype=torch.long\n",
    "    ).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    trained_model.eval()\n",
    "    \n",
    "    for i in range(num_test_users):\n",
    "        sample_user_id = sample_user_ids[i]\n",
    "        sample_user_idx = graph_builder.user_to_idx[sample_user_id]\n",
    "        \n",
    "        \n",
    "        print(f\"USER {i+1}: {sample_user_id}\")\n",
    "        print(f\"User node index: {sample_user_idx}\")\n",
    "        \n",
    "        \n",
    "        # Get movies this user interacted with (ground truth)\n",
    "        user_movies = []\n",
    "        for rel_type, edges in graph_builder.edges.items():\n",
    "            user_movies.extend([\n",
    "                graph_builder.idx_to_movie.get(dst, \"Unknown\")\n",
    "                for src, dst in edges if src == sample_user_idx\n",
    "            ])\n",
    "        \n",
    "        if user_movies:\n",
    "            print(f\"\\nMovies user interacted with:\")\n",
    "            for j, movie in enumerate(user_movies[:5], 1):\n",
    "                print(f\"  {j}. {movie}\")\n",
    "            if len(user_movies) > 5:\n",
    "                print(f\"  ... and {len(user_movies) - 5} more\")\n",
    "        else:\n",
    "            print(\"\\nNo interaction history found for this user\")\n",
    "        \n",
    "        # Get model recommendations\n",
    "        recommendations = trained_model.predict_top_k(\n",
    "            edge_index=graph_data['edge_index'],\n",
    "            edge_type=graph_data['edge_type'],\n",
    "            user_idx=sample_user_idx,\n",
    "            candidate_movies=all_movie_indices,\n",
    "            k=10\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTop-10 RGCN Recommendations:\")\n",
    "        print(f\"{'Rank':<6} {'Movie Title':<50} {'Score':<10}\")\n",
    "        \n",
    "        \n",
    "        for rank, rec in enumerate(recommendations, 1):\n",
    "            movie_idx = rec['movie_node_idx']\n",
    "            movie_name = graph_builder.idx_to_movie.get(movie_idx, \"Unknown\")\n",
    "            score = rec['score']\n",
    "            \n",
    "            # Truncate long names\n",
    "            if len(movie_name) > 47:\n",
    "                movie_name = movie_name[:47] + \"...\"\n",
    "            \n",
    "            print(f\"{rank:<6} {movie_name:<50} {score:>8.4f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"TESTING COMPLETE\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo users found in graph!\")\n",
    "    print(\"Check if graph was built correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1299443-34ea-47d3-a903-682221ecbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: EVALUATE ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import evaluator\n",
    "from scripts.evaluator import RecommenderEvaluator\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "test_processor = INSPIREDDataProcessor(dataset_dir=\"data\")\n",
    "test_processor.user_to_idx = data_processor.user_to_idx\n",
    "test_processor.movie_to_idx = data_processor.movie_to_idx\n",
    "test_processor.idx_to_user = data_processor.idx_to_user\n",
    "test_processor.idx_to_movie = data_processor.idx_to_movie\n",
    "test_processor.load_dialogs(split=\"test\", max_dialogs=None)\n",
    "\n",
    "# Build ground truth\n",
    "ground_truth = {}\n",
    "for user_idx, item_idx, rating in test_processor.interactions:\n",
    "    if user_idx not in ground_truth:\n",
    "        ground_truth[user_idx] = set()\n",
    "    ground_truth[user_idx].add(item_idx)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RecommenderEvaluator(k_values=[1, 3, 5, 10])\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "test_users = list(ground_truth.keys())[:100]\n",
    "\n",
    "print(f\"Evaluating on {len(test_users)} users...\\n\")\n",
    "\n",
    "for user_idx in tqdm(test_users, desc=\"Evaluating\"):\n",
    "    if user_idx not in ground_truth or not ground_truth[user_idx]:\n",
    "        continue\n",
    "    \n",
    "    # Get all candidate items\n",
    "    all_items_tensor = torch.tensor(all_items_list, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get recommendations\n",
    "    recs = trained_model.predict_top_k(\n",
    "        user_idx=user_idx,\n",
    "        candidate_items=all_items_tensor,\n",
    "        k=10\n",
    "    )\n",
    "    \n",
    "    recommended_ids = [rec['item_idx'] for rec in recs]\n",
    "    \n",
    "    predictions.append({\n",
    "        'recommended': recommended_ids,\n",
    "        'ground_truth': ground_truth[user_idx]\n",
    "    })\n",
    "\n",
    "# Evaluate\n",
    "results = evaluator.evaluate_batch(predictions)\n",
    "evaluator.print_results(dataset_name=\"NCF Test Set\", num_samples=len(predictions))\n",
    "\n",
    "# Save results\n",
    "evaluator.save_results(\n",
    "    output_path=\"models/ncf_checkpoints/evaluation_results.json\",\n",
    "    model_name=\"NCF\",\n",
    "    metadata={'num_users': num_users, 'num_items': num_items}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

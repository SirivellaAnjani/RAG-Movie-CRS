{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c39ddd4-0108-44b4-a954-143d06fff4bd",
   "metadata": {},
   "source": [
    "# Integrating NCF as an External Recommenders\n",
    "\n",
    "In this notebook, the LLM + RAG + NCF model is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ade436-9b3f-4a5c-b61c-af7ce77ba61c",
   "metadata": {},
   "source": [
    "## Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e05eb1b-492c-4b7a-9d79-85a88c25a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\91953\\Documents\\GitHub\\RAG-Movie-CRS\n",
      "Current Working Directory: C:\\Users\\91953\\Documents\\GitHub\\RAG-Movie-CRS\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check Current Working Directory\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(\"Project Root:\", project_root)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47eb83b-7d28-4cc3-a727-528f4bcde1c7",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a2864c-ed76-4bfb-a3a9-fe6e2e36c842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: ollama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: llama-index-core in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.14.8)\n",
      "Requirement already satisfied: llama-index-embeddings-ollama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (0.8.4)\n",
      "Requirement already satisfied: llama-index-llms-ollama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: chromadb in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: pypdf in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (6.4.0)\n",
      "Requirement already satisfied: nbimport in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (0.0.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (4.57.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (1.7.2)\n",
      "Requirement already satisfied: torch in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (0.24.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\91953\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from ollama->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from ollama->-r requirements.txt (line 2)) (2.12.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\91953\\anaconda3\\lib\\site-packages (from tqdm->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (3.13.2)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2025.10.0)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2.11.5)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (3.9.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (10.3.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 4)) (2.0.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-core->-r requirements.txt (line 4)) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (1.22.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\91953\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core->-r requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core->-r requirements.txt (line 4)) (3.11)\n",
      "Requirement already satisfied: pytest-asyncio>=0.23.8 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from llama-index-embeddings-ollama->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (0.38.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (3.11.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 8)) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 4)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 4)) (2025.11.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 13)) (1.14.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\91953\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\91953\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91953\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 8)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 8)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 8)) (0.28.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (0.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\91953\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core->-r requirements.txt (line 4)) (8.3.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\91953\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\91953\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (25.9.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\91953\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (6.33.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 8)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 8)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 8)) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 8)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 8)) (0.59b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 2)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: pytest<10,>=8.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (9.0.0)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama->-r requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 4)) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 8)) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (1.2.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27->ollama->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 8)) (3.5.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core->-r requirements.txt (line 4)) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\91953\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 8)) (3.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753acff-d903-4da6-83eb-e2fc8641220b",
   "metadata": {},
   "source": [
    "### RAG, LLM, Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93160c4-d716-4b38-a400-7fc23d8a2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Import LlamaIndex components\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from scripts.ncf_recommender import NCFModel, INSPIREDDataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04d93570-cfd3-4524-8c5f-e504ecacd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    request_timeout=300.0,\n",
    ")\n",
    "\n",
    "# Initialize the LLM with optimized settings\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2:1B\",\n",
    "    request_timeout=300.0,\n",
    "    temperature=0.1,\n",
    "    additional_kwargs={\"num_gpu\": 0}  # Forcing CPU usage\n",
    ")\n",
    "\n",
    "# Set global configurations\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab9c9a-0588-4d21-b930-e7876b4a6b22",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "375fdbb0-77aa-4baa-a2ee-6bc98a2d0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions:\n",
    "    - Load INSPIRED dataset from TSV \n",
    "    - Convert to documents\n",
    "\n",
    "Args:\n",
    "    - data_path: Path to the TSV file\n",
    "    - max_rows: Maximum number of rows to load (None = load all rows)\n",
    "'''\n",
    "\n",
    "def load_inspired_dataset(data_path, max_rows=None):\n",
    "    \n",
    "    if not Path(data_path).exists():\n",
    "        raise FileNotFoundError(f\"INSPIRED dataset not found at '{data_path}'.\")\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    # First pass: count total rows for progress bar\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        total_rows = sum(1 for _ in f) - 1  # Subtract header row\n",
    "    \n",
    "    if max_rows is not None:\n",
    "        total_rows = min(total_rows, max_rows)\n",
    "    \n",
    "    # Load the TSV data\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        \n",
    "        for idx, row in enumerate(tqdm(reader, total=total_rows, desc=\"Loading data\")):\n",
    "            # Stop if we've reached max_rows\n",
    "            if max_rows is not None and idx >= max_rows:\n",
    "                break\n",
    "                \n",
    "            # Extract conversation information from TSV\n",
    "            dialog_id = row.get('dialog_id', '')\n",
    "            turn_id = row.get('turn_id', '')\n",
    "            utterance = row.get('utterance', '')\n",
    "            speaker = row.get('speaker', '')\n",
    "            movie_name = row.get('movies', '')\n",
    "            \n",
    "            # Create document text\n",
    "            doc_text = f\"Speaker: {speaker}\\nUtterance: {utterance}\\n\"\n",
    "            \n",
    "            if movie_name:\n",
    "                doc_text += f\"Movie mentioned: {movie_name}\\n\"\n",
    "            \n",
    "            # Create metadata\n",
    "            metadata = {\n",
    "                \"dialog_id\": dialog_id,\n",
    "                \"turn_id\": turn_id,\n",
    "                \"speaker\": speaker,\n",
    "                \"movie_name\": movie_name if movie_name else \"None\"\n",
    "            }\n",
    "            \n",
    "            # Create Document object\n",
    "            doc = Document(text=doc_text, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} turns from INSPIRED dataset\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240deead-b273-4a61-afc6-0929a0fcfe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions:\n",
    "    - Load the unprocessed INSPIRED movie database for reference\n",
    "'''\n",
    "\n",
    "def load_movie_database(dataset_dir=\"data\"):\n",
    "    \n",
    "    movie_db_path = Path(dataset_dir) / \"raw\" / \"movie_database.tsv\"\n",
    "    \n",
    "    if not movie_db_path.exists():\n",
    "        print(f\"Movie database not found at {movie_db_path}\")\n",
    "        return {}\n",
    "    \n",
    "    movies = {}\n",
    "    with open(movie_db_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            movie_id = row.get('movieId', '')\n",
    "            movies[movie_id] = row\n",
    "    \n",
    "    print(f\"Loaded {len(movies)} movies from movie database\")\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15d594-c387-4054-a287-7ca534fc58ab",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1fdaa-add7-46aa-bafb-741daff6d08b",
   "metadata": {},
   "source": [
    "### Create Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93467a66-007e-40c0-bda4-2d928ec924dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function:\n",
    "    - Load INSPIRED dataset\n",
    "    - Create vector index\n",
    "\n",
    "Args:\n",
    "    - dataset_dir: Directory containing the dataset\n",
    "    - split: Which split to use (train, dev, test)\n",
    "    - max_rows: Maximum number of rows to load (None for all rows)\n",
    "    - load_movie_db: Whether to load movie database (needed for external recommenders)\n",
    "\n",
    "Returns:\n",
    "    - index: VectorStoreIndex for RAG\n",
    "    - movie_db: Movie database (if load_movie_db=True), else None\n",
    "'''\n",
    "\n",
    "def load_and_index_documents(dataset_dir=\"data\", split=\"train\", max_rows=None, load_movie_db=True):\n",
    "        \n",
    "    # Construct path to the data file \n",
    "    data_path = Path(dataset_dir) / \"processed\" / f\"{split}.tsv\"\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found at {data_path}. \\nCheck for typos.\"\n",
    "        )\n",
    "    \n",
    "    # Load INSPIRED documents with max_rows limit\n",
    "    docs = load_inspired_dataset(data_path, max_rows=max_rows)\n",
    "    \n",
    "    if not docs:\n",
    "        raise ValueError(\"No documents loaded from INSPIRED dataset\")\n",
    "    \n",
    "    # load movie database for reference\n",
    "    # Used by external recommenders\n",
    "    movie_db = load_movie_database(dataset_dir)\n",
    "    \n",
    "    # Load documents, then\n",
    "    # Build vector index from documents\n",
    "    print(\"Building vector index...\")\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        docs, \n",
    "        embed_model=embed_model,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    return index, movie_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395c9f2-ec6f-456f-8b65-46ce6a8b1f0e",
   "metadata": {},
   "source": [
    "### Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b766a5b6-32c7-499a-a590-83cb225d56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function:\n",
    "    - Create query engine with specified retrieval parameters\n",
    "'''\n",
    "\n",
    "def create_query_engine(index, similarity_top_k=5):\n",
    "    \n",
    "    query_engine = index.as_query_engine(\n",
    "        llm=llm,\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        response_mode=\"compact\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76920e49-6e58-43d3-8fac-92967af05a0f",
   "metadata": {},
   "source": [
    "### Adapt External Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9a3f1-6d5b-4724-b297-5933df7f54df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5702aca5-eb32-4661-b4ab-98f28ceadc0c",
   "metadata": {},
   "source": [
    "### Interactive Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ea846df-d92c-445b-8029-ba30720b610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Interactive conversation with history tracking\n",
    "'''\n",
    "def interactive_conversation_with_history():\n",
    "    \n",
    "    try:\n",
    "        # Load documents and create index\n",
    "        print(\"\\nLoading INSPIRED dataset...\")\n",
    "        index, _ = load_and_index_documents(split=\"train\", max_rows=20)\n",
    "        \n",
    "        # Create chat engine instead of query engine\n",
    "        chat_engine = index.as_chat_engine(\n",
    "            llm=llm,\n",
    "            similarity_top_k=5,\n",
    "            chat_mode=\"context\"  # Use context mode for history\n",
    "        )\n",
    "        \n",
    "        \n",
    "        print(\"|| MovieCRS is Ready ||\")\n",
    "        \n",
    "        print(\"\\nYou can now ask for movie recommendations.\")\n",
    "        print(\"Type 'quit', 'exit', or 'q' to end the conversation.\\n\")\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q', 'bye']:\n",
    "                print(\"\\nMovieCRS: Exiting...\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Use chat() instead of query() - it handles history automatically\n",
    "                print(\"\\nMovieCRS: \", end=\"\", flush=True)\n",
    "                response = chat_engine.chat(user_input)\n",
    "                \n",
    "                # Print only the response text\n",
    "                print(f\"{response.response}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\\n\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"System Error: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dbf603-dce1-4b36-9738-c79df2261cb3",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c777faf-fe1f-4a14-aa16-7752c781dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAG Pipeline with INSPIRED Dataset...\n",
      "\n",
      "Loading INSPIRED dataset...\n",
      "System Error: name 'csv' is not defined\n",
      "\n",
      "System failed to start.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Starting RAG Pipeline with INSPIRED Dataset...\")\n",
    "    \n",
    "    # Start interactive conversation with history\n",
    "    success = interactive_conversation_with_history()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"\\nSystem failed to start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b9987-6e7d-43af-94c5-afda31721d8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84320e56-f528-4e81-b0f3-4ed66ee7f34f",
   "metadata": {},
   "source": [
    "### Standard Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e5f9e-2773-4b83-a767-ad49758cc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluator import RecommenderEvaluator\n",
    "\n",
    "print(\"RUNNING NCF EVALUATION\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RecommenderEvaluator(k_values=[1, 3, 5, 10])\n",
    "\n",
    "# Load test data\n",
    "data_processor = INSPIREDDataProcessor(dataset_dir=\"data\")\n",
    "data_processor.load_movie_database()\n",
    "data_processor.load_dialogs(split=\"test\", max_dialogs=None)\n",
    "\n",
    "# Build ground truth\n",
    "ground_truth = {}\n",
    "for user_idx, item_idx, rating in data_processor.interactions:\n",
    "    if user_idx not in ground_truth:\n",
    "        ground_truth[user_idx] = set()\n",
    "    ground_truth[user_idx].add(item_idx)\n",
    "\n",
    "# Prepare predictions list\n",
    "predictions = []\n",
    "\n",
    "# Get all movie indices\n",
    "all_movie_indices = torch.tensor(\n",
    "    list(ncf_mappings['movie_to_idx'].values()),\n",
    "    dtype=torch.long\n",
    ").to(device)\n",
    "\n",
    "# Evaluate users\n",
    "test_users = list(ground_truth.keys())[:100]\n",
    "print(f\"\\nEvaluating on {len(test_users)} users...\\n\")\n",
    "\n",
    "for user_idx in tqdm(test_users, desc=\"Generating predictions\"):\n",
    "    if user_idx not in ground_truth or not ground_truth[user_idx]:\n",
    "        continue\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = ncf_model.predict_top_k(\n",
    "        user_idx=user_idx,\n",
    "        candidate_items=all_movie_indices,\n",
    "        k=10\n",
    "    )\n",
    "    \n",
    "    recommended_ids = [rec['item_idx'] for rec in recommendations]\n",
    "    \n",
    "    # Add to predictions\n",
    "    predictions.append({\n",
    "        'recommended': recommended_ids,\n",
    "        'ground_truth': ground_truth[user_idx]\n",
    "    })\n",
    "\n",
    "# Evaluate batch\n",
    "results = evaluator.evaluate_batch(predictions)\n",
    "\n",
    "# Print results\n",
    "evaluator.print_results(\n",
    "    dataset_name=\"NCF Test Set\",\n",
    "    num_samples=len(predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066be444-08ea-4383-864d-0aa8de4bbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_path = \"models/ncf_checkpoints/evaluation_results.json\"\n",
    "\n",
    "evaluator.save_results(\n",
    "    output_path=output_path,\n",
    "    model_name=\"NCF+LLM+RAG\",\n",
    "    metadata={\n",
    "        'num_users': ncf_model.num_users,\n",
    "        'num_items': ncf_model.num_items,\n",
    "        'embedding_dim': checkpoint['embedding_dim'],\n",
    "        'test_samples': len(predictions)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Show results table\n",
    "results_table = evaluator.get_summary_table()\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4846c143-d2ec-46a1-bb5c-a50ff1370ce9",
   "metadata": {},
   "source": [
    "### Contextual Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6eea67-9f67-46e2-8941-d0983ede6092",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7b83c-fcb8-44f2-ba03-8a2d864aa9c2",
   "metadata": {},
   "source": [
    "## DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44335276-eccc-4ad2-9df6-8d1e6404f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\91953\\JupyterNotebooks\\RAG-2-optimized\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check Current Working Directory\n",
    "'''\n",
    "import os\n",
    "\n",
    "# Change Working Directory if needed \n",
    "os.chdir(\"C:/Users/91953/JupyterNotebooks/RAG-2-optimized\")\n",
    "\n",
    "# Check if in correct directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e672b5ce-a84c-441f-8ceb-ea10d1a64a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CHECKING INSPIRED DATASET COLUMN NAMES\n",
      "============================================================\n",
      "\n",
      "Dataset directory: data\n",
      "Checking 4 files...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Movie Database\n",
      "File: data/raw/movie_database.tsv\n",
      "============================================================\n",
      "✓ Found 30 columns:\n",
      "  1. title\n",
      "  2. year\n",
      "  3. trailer_duration\n",
      "  4. actors\n",
      "  5. awards\n",
      "  6. box_office\n",
      "  7. country\n",
      "  8. director\n",
      "  9. dvd_release\n",
      "  10. genre\n",
      "  11. imdb_id\n",
      "  12. imdb_type\n",
      "  13. imdb_votes\n",
      "  14. language\n",
      "  15. long_plot\n",
      "  16. movie_runtime\n",
      "  17. poster\n",
      "  18. production\n",
      "  19. rated\n",
      "  20. rating\n",
      "  21. release_date\n",
      "  22. short_plot\n",
      "  23. video_id\n",
      "  24. writer\n",
      "  25. youtube_comment\n",
      "  26. youtube_dislike\n",
      "  27. youtube_favorite\n",
      "  28. youtube_like\n",
      "  29. youtube_link\n",
      "  30. youtube_view\n",
      "\n",
      "Sample row (first row):\n",
      "  title: Antlers\n",
      "  year: 2020\n",
      "  trailer_duration: 123.0\n",
      "  actors: Keri Russell, Jesse Plemons, Jeremy T. Thomas, Gra...\n",
      "  awards: \n",
      "\n",
      "============================================================\n",
      "Training Dialog Data\n",
      "File: data/processed/train.tsv\n",
      "============================================================\n",
      "✓ Found 19 columns:\n",
      "  1. dialog_id\n",
      "  2. utt_id\n",
      "  3. speaker\n",
      "  4. turn_id\n",
      "  5. text\n",
      "  6. text_with_placeholder\n",
      "  7. movies\n",
      "  8. genres\n",
      "  9. people_names\n",
      "  10. movie_dict\n",
      "  11. genre_dict\n",
      "  12. actor_dict\n",
      "  13. director_dict\n",
      "  14. others_dict\n",
      "  15. expert_label\n",
      "  16. second_label\n",
      "  17. fine_label\n",
      "  18. coarse_label\n",
      "  19. movie_id\n",
      "\n",
      "Sample row (first row):\n",
      "  dialog_id: 20191127-210600_875_live.pkl\n",
      "  utt_id: 1\n",
      "  speaker: RECOMMENDER\n",
      "  turn_id: 1\n",
      "  text: Hi There!\n",
      "\n",
      "============================================================\n",
      "Test Dialog Data\n",
      "File: data/processed/test.tsv\n",
      "============================================================\n",
      "✓ Found 19 columns:\n",
      "  1. dialog_id\n",
      "  2. utt_id\n",
      "  3. speaker\n",
      "  4. turn_id\n",
      "  5. text\n",
      "  6. text_with_placeholder\n",
      "  7. movies\n",
      "  8. genres\n",
      "  9. people_names\n",
      "  10. movie_dict\n",
      "  11. genre_dict\n",
      "  12. actor_dict\n",
      "  13. director_dict\n",
      "  14. others_dict\n",
      "  15. expert_label\n",
      "  16. second_label\n",
      "  17. fine_label\n",
      "  18. coarse_label\n",
      "  19. movie_id\n",
      "\n",
      "Sample row (first row):\n",
      "  dialog_id: 20191127-224739_530_live.pkl\n",
      "  utt_id: 1\n",
      "  speaker: RECOMMENDER\n",
      "  turn_id: 1\n",
      "  text: Hi!\n",
      "\n",
      "============================================================\n",
      "Dev Dialog Data\n",
      "File: data/processed/dev.tsv\n",
      "============================================================\n",
      "✓ Found 19 columns:\n",
      "  1. dialog_id\n",
      "  2. utt_id\n",
      "  3. speaker\n",
      "  4. turn_id\n",
      "  5. text\n",
      "  6. text_with_placeholder\n",
      "  7. movies\n",
      "  8. genres\n",
      "  9. people_names\n",
      "  10. movie_dict\n",
      "  11. genre_dict\n",
      "  12. actor_dict\n",
      "  13. director_dict\n",
      "  14. others_dict\n",
      "  15. expert_label\n",
      "  16. second_label\n",
      "  17. fine_label\n",
      "  18. coarse_label\n",
      "  19. movie_id\n",
      "\n",
      "Sample row (first row):\n",
      "  dialog_id: 20191202-103716_425_live.pkl\n",
      "  utt_id: 1\n",
      "  speaker: RECOMMENDER\n",
      "  turn_id: 1\n",
      "  text: Hello how are you today?\n",
      "\n",
      "============================================================\n",
      "DONE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def check_columns(file_path, file_description):\n",
    "    \"\"\"Check and print column names from a TSV file\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{file_description}\")\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"❌ FILE NOT FOUND!\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            columns = reader.fieldnames\n",
    "            \n",
    "            print(f\"✓ Found {len(columns)} columns:\")\n",
    "            for i, col in enumerate(columns, 1):\n",
    "                print(f\"  {i}. {col}\")\n",
    "            \n",
    "            # Print first row as sample\n",
    "            print(f\"\\nSample row (first row):\")\n",
    "            first_row = next(reader, None)\n",
    "            if first_row:\n",
    "                for col in columns[:5]:  # Show first 5 columns\n",
    "                    value = first_row.get(col, '')\n",
    "                    # Truncate long values\n",
    "                    if len(str(value)) > 50:\n",
    "                        value = str(value)[:50] + \"...\"\n",
    "                    print(f\"  {col}: {value}\")\n",
    "            \n",
    "            return columns\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHECKING INSPIRED DATASET COLUMN NAMES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define file paths - UPDATE THESE TO YOUR ACTUAL PATHS\n",
    "    dataset_dir = \"data\"  # Change this if needed\n",
    "    \n",
    "    files_to_check = [\n",
    "        (f\"{dataset_dir}/raw/movie_database.tsv\", \"Movie Database\"),\n",
    "        (f\"{dataset_dir}/processed/train.tsv\", \"Training Dialog Data\"),\n",
    "        (f\"{dataset_dir}/processed/test.tsv\", \"Test Dialog Data\"),\n",
    "        (f\"{dataset_dir}/processed/dev.tsv\", \"Dev Dialog Data\"),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nDataset directory: {dataset_dir}\")\n",
    "    print(f\"Checking {len(files_to_check)} files...\\n\")\n",
    "    \n",
    "    for file_path, description in files_to_check:\n",
    "        check_columns(file_path, description)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DONE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f50233-dcae-426d-b6d9-d0e1ffb0e653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
